{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNqgX9Y8cHTFMVAo8TMCkHC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmirJlr/Deep-Learning-FUM/blob/master/00-tensorflow-first-steps/AutoGrad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### TensorFlow is not limited to build a neural network. Behind the scenes, TensorFlow is a tensor library with **automatic differentiation** capability. Hence you can easily use it to solve a numerical optimization problem with gradient descent\n",
        "\n",
        "### Overview\n",
        "- Autograd in TensorFlow\n",
        "\n",
        "- Using Autograd to Solve a Math Puzzle"
      ],
      "metadata": {
        "id": "xo-g0QRiNTr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autograd in TensorFlow"
      ],
      "metadata": {
        "id": "dC3NVfu8N2rZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbG2Ra9dM8Av",
        "outputId": "1d690cdc-268a-4576-82a7-e3a53f3341d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
            "(3,)\n",
            "<dtype: 'int32'>\n"
          ]
        }
      ],
      "source": [
        "### create a constant matrix ###\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "x = tf.constant([1, 2, 3])\n",
        "print(x)\n",
        "print(x.shape)\n",
        "print(x.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Creating variables in TensorFlow ###\n",
        "import tensorflow as tf\n",
        "\n",
        "x = tf.Variable([1, 2, 3])\n",
        "print(x)\n",
        "print(x.shape)\n",
        "print(x.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODBAgvP5OG1L",
        "outputId": "35efb1a4-3508-4a06-b836-55c98e23c5e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'Variable:0' shape=(3,) dtype=int32, numpy=array([1, 2, 3], dtype=int32)>\n",
            "(3,)\n",
            "<dtype: 'int32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only **difference between variables and constants** is the former allows the value to change while the latter is immutable. This distinction is important when you run a **gradient tape** as follow:"
      ],
      "metadata": {
        "id": "s4IRxxm0Otht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(3.6)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = x*x\n",
        "\n",
        "dy = tape.gradient(y, x)\n",
        "print(dy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvPZZjptOheO",
        "outputId": "b4f20457-c025-48bc-8462-5dd616bbd695"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(7.2, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using autograd to Solve a Math Puzzle\n",
        "\n",
        "\n",
        " You may use gradient descent to solve some math puzzles as well. For example, the following problem:\n",
        "\n",
        "```\n",
        " [ A ]  +  [ B ]  =  9\n",
        "   +         -\n",
        " [ C ]  -  [ D ]  =  1\n",
        "   =         =\n",
        "   8         2\n",
        "\n",
        "```\n",
        "\n",
        "In other words, to find the values of\n",
        "A, B, C, D such that:\n",
        "\n",
        "- A + B = 9\n",
        "- C - D = 1\n",
        "- A + C = 8\n",
        "- B - D = 2\n",
        "\n",
        "#### This can also be solved using autograd, as follows:"
      ],
      "metadata": {
        "id": "S9hj41BoQEMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "A = tf.Variable(random.random())\n",
        "B = tf.Variable(random.random())\n",
        "C = tf.Variable(random.random())\n",
        "D = tf.Variable(random.random())\n",
        "\n",
        "# Gradient descent loop\n",
        "EPOCHS = 1000\n",
        "\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.1)\n",
        "\n",
        "for _ in range(EPOCHS):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y1 = A + B - 9\n",
        "        y2 = C - D - 1\n",
        "        y3 = A + C - 8\n",
        "        y4 = B - D - 2\n",
        "        sqerr = y1*y1 + y2*y2 + y3*y3 + y4*y4\n",
        "    gradA, gradB, gradC, gradD = tape.gradient(sqerr, [A, B, C, D])\n",
        "\n",
        "    optimizer.apply_gradients([(gradA, A), (gradB, B), (gradC, C), (gradD, D)])\n",
        "\n",
        "print(A)\n",
        "print(B)\n",
        "print(C)\n",
        "print(D)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMY1xhuaQFF7",
        "outputId": "78b10a64-af18-40be-8b92-bf8803fed98c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 4001 calls to <function _BaseOptimizer._update_step_xla at 0x7f00931fb400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.7298384>\n",
            "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.2701626>\n",
            "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.2701623>\n",
            "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.2701626>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The above code defines the four unknowns as variables with a random initial value.\n",
        "\n",
        "- Then you compute the result of the four equations and compare it to the expected answer.\n",
        "\n",
        "- You then sum up the squared error and ask TensorFlow to minimize it.\n",
        "\n",
        "- The minimum possible square error is zero, attained when our solution exactly fits the problem.\n",
        "\n",
        "- Note the way the gradient tape is asked to produce the gradient: You ask the gradient of sqerr respective to A, B, C, and D. Hence four gradients are found. You then apply each gradient to the respective variables in each iteration. Rather than looking for the gradient in four different calls to tape.gradient(), this is required in TensorFlow because the gradient sqerr can only be recalled once by default.\n",
        "\n"
      ],
      "metadata": {
        "id": "KPAzIph4RnQM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rfGaaVDLQPlV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}