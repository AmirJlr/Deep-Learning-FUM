{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtXjgHA+73evIjXT8PRTOg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmirJlr/Deep-Learning-FUM/blob/master/00-tensorflow-first-steps/TensorFlowHandsOn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing a training loop from scratch in TensorFlow\n",
        "\n",
        "Writing low-level training & evaluation loops in TensorFlow."
      ],
      "metadata": {
        "id": "wN9lGB3W_u7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "\n",
        "# This guide can only be run with the TensorFlow backend.\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ifnbdlAw_qs8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MNIST model"
      ],
      "metadata": {
        "id": "fwkyik66LjMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    inputs = keras.Input(shape=(784,), name='digits')\n",
        "    x1 = keras.layers.Dense(64, activation = 'relu')(inputs)\n",
        "    x2 = keras.layers.Dense(64, activation = 'relu')(x1)\n",
        "    outputs = keras.layers.Dense(10, name='prediction')(x2)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = get_model()"
      ],
      "metadata": {
        "id": "RcNJXO2IBBXg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Let's train it using `mini-batch gradient` with a custom training loop.\n",
        "\n",
        "\n",
        " we're going to need an `optimizer`, a `loss function`, and a `dataset`:"
      ],
      "metadata": {
        "id": "X3U7XLj6M0Xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "# Loss Function\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Prepare Data\n",
        "batch_size = 32\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgynBKLTMoUD",
        "outputId": "a1154d4c-0f1d-4bba-e35b-30a48cef6fb8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.reshape(x_train, (-1, 784))\n",
        "x_test = np.reshape(x_test, (-1, 784))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWFmEoHvOLcO",
        "outputId": "fafbe3aa-fd4b-4e70-cb10-2d4bd8925dd5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reserve 10,000 samples for validation.\n",
        "x_val = x_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "x_train = x_train[:-10000]\n",
        "y_train = y_train[:-10000]"
      ],
      "metadata": {
        "id": "Q5cVP-HCOjeh"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zgvU6G9O0s-",
        "outputId": "37b772b2-5ec7-4cdb-dbe9-2ae20ce9076d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 784)\n",
            "(50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0], x_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxu_oiWrPNEA",
        "outputId": "6a9d99d2-0242-4286-ef51-d20721da6b9b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5,\n",
              " array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
              "        126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
              "        253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
              "        253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
              "        253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
              "        253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
              "        183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
              "        229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
              "        221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
              "        213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
              "        219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
              "        226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0], dtype=uint8))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Prepare Training Data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIW9X8EeO9yd",
        "outputId": "05e54223-502b-4a09-c7af-0cacc483e817"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TensorSliceDataset element_spec=(TensorSpec(shape=(784,), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyQczUtyPdNZ",
        "outputId": "faa0ab40-0a42-4d2e-c8b6-0a810aa01d28"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(None, 784), dtype=tf.uint8, name=None), TensorSpec(shape=(None,), dtype=tf.uint8, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Prepare the validation dataset.\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "Iltp65QnPkz_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling a model inside a **GradientTape scope** enables you to **retrieve the gradients of the trainable weights of the layer with respect to a loss value**.\n",
        "\n",
        " Using an **optimizer** instance, you can use these gradients to **update these variables** (which you can retrieve using model.trainable_weights).\n",
        "\n",
        "\n",
        "#### Here's our training loop, step by step:\n",
        "\n",
        "- We open a for loop that iterates over epochs\n",
        "\n",
        "- For each epoch, we open a for loop that iterates over the dataset, in batches\n",
        "\n",
        "- For each batch, we open a GradientTape() scope\n",
        "\n",
        "- Inside this scope, we call the model (forward pass) and compute the loss\n",
        "Outside the scope, we retrieve the gradients of the weights of the model with regard to the loss\n",
        "\n",
        "- Finally, we use the optimizer to update the weights of the model based on the gradients"
      ],
      "metadata": {
        "id": "vN0DOzgiP0dC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHES = 3\n",
        "\n",
        "for epoch in range(1, EPOCHES+1):\n",
        "    print(f'\\nStart of epoch {epoch}')\n",
        "\n",
        "    # iterate over datadet in batchs\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "        # open a GradientTape() scope\n",
        "        with tf.GradientTape() as tape:\n",
        "            # call the model (forward pass) and compute the loss\n",
        "            logits= model(x_batch_train, training = True)\n",
        "\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "        # retrieve the gradients of the weights of the model with regard to the loss\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "\n",
        "        # use the optimizer to update the weights of the model based on the gradients\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Log every 100 batches.\n",
        "        if step % 100 == 0:\n",
        "            print(\n",
        "                f\"Training loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"\n",
        "            )\n",
        "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYdGhqwfPsz7",
        "outputId": "eddf418a-c53c-4e8a-c173-cec28ed895e5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Star of epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7c003f38f910> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7c003f38f910> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for 1 batch) at step 0: 120.1026\n",
            "Seen so far: 32 samples\n",
            "Training loss (for 1 batch) at step 100: 2.4283\n",
            "Seen so far: 3232 samples\n",
            "Training loss (for 1 batch) at step 200: 2.0205\n",
            "Seen so far: 6432 samples\n",
            "Training loss (for 1 batch) at step 300: 1.7353\n",
            "Seen so far: 9632 samples\n",
            "Training loss (for 1 batch) at step 400: 0.6094\n",
            "Seen so far: 12832 samples\n",
            "Training loss (for 1 batch) at step 500: 1.8939\n",
            "Seen so far: 16032 samples\n",
            "Training loss (for 1 batch) at step 600: 1.6675\n",
            "Seen so far: 19232 samples\n",
            "Training loss (for 1 batch) at step 700: 0.4265\n",
            "Seen so far: 22432 samples\n",
            "Training loss (for 1 batch) at step 800: 1.8246\n",
            "Seen so far: 25632 samples\n",
            "Training loss (for 1 batch) at step 900: 0.7020\n",
            "Seen so far: 28832 samples\n",
            "Training loss (for 1 batch) at step 1000: 1.0976\n",
            "Seen so far: 32032 samples\n",
            "Training loss (for 1 batch) at step 1100: 1.0294\n",
            "Seen so far: 35232 samples\n",
            "Training loss (for 1 batch) at step 1200: 0.6754\n",
            "Seen so far: 38432 samples\n",
            "Training loss (for 1 batch) at step 1300: 0.6110\n",
            "Seen so far: 41632 samples\n",
            "Training loss (for 1 batch) at step 1400: 0.5952\n",
            "Seen so far: 44832 samples\n",
            "Training loss (for 1 batch) at step 1500: 0.4821\n",
            "Seen so far: 48032 samples\n",
            "\n",
            "Star of epoch 1\n",
            "Training loss (for 1 batch) at step 0: 0.8216\n",
            "Seen so far: 32 samples\n",
            "Training loss (for 1 batch) at step 100: 0.4211\n",
            "Seen so far: 3232 samples\n",
            "Training loss (for 1 batch) at step 200: 0.3352\n",
            "Seen so far: 6432 samples\n",
            "Training loss (for 1 batch) at step 300: 0.0974\n",
            "Seen so far: 9632 samples\n",
            "Training loss (for 1 batch) at step 400: 1.0269\n",
            "Seen so far: 12832 samples\n",
            "Training loss (for 1 batch) at step 500: 0.3330\n",
            "Seen so far: 16032 samples\n",
            "Training loss (for 1 batch) at step 600: 0.3543\n",
            "Seen so far: 19232 samples\n",
            "Training loss (for 1 batch) at step 700: 0.0821\n",
            "Seen so far: 22432 samples\n",
            "Training loss (for 1 batch) at step 800: 0.2801\n",
            "Seen so far: 25632 samples\n",
            "Training loss (for 1 batch) at step 900: 0.0243\n",
            "Seen so far: 28832 samples\n",
            "Training loss (for 1 batch) at step 1000: 0.2010\n",
            "Seen so far: 32032 samples\n",
            "Training loss (for 1 batch) at step 1100: 0.1467\n",
            "Seen so far: 35232 samples\n",
            "Training loss (for 1 batch) at step 1200: 0.1597\n",
            "Seen so far: 38432 samples\n",
            "Training loss (for 1 batch) at step 1300: 0.9201\n",
            "Seen so far: 41632 samples\n",
            "Training loss (for 1 batch) at step 1400: 0.3348\n",
            "Seen so far: 44832 samples\n",
            "Training loss (for 1 batch) at step 1500: 0.6360\n",
            "Seen so far: 48032 samples\n",
            "\n",
            "Star of epoch 2\n",
            "Training loss (for 1 batch) at step 0: 0.7347\n",
            "Seen so far: 32 samples\n",
            "Training loss (for 1 batch) at step 100: 0.0615\n",
            "Seen so far: 3232 samples\n",
            "Training loss (for 1 batch) at step 200: 0.0891\n",
            "Seen so far: 6432 samples\n",
            "Training loss (for 1 batch) at step 300: 0.1559\n",
            "Seen so far: 9632 samples\n",
            "Training loss (for 1 batch) at step 400: 0.3093\n",
            "Seen so far: 12832 samples\n",
            "Training loss (for 1 batch) at step 500: 0.2442\n",
            "Seen so far: 16032 samples\n",
            "Training loss (for 1 batch) at step 600: 0.1553\n",
            "Seen so far: 19232 samples\n",
            "Training loss (for 1 batch) at step 700: 0.0769\n",
            "Seen so far: 22432 samples\n",
            "Training loss (for 1 batch) at step 800: 0.0907\n",
            "Seen so far: 25632 samples\n",
            "Training loss (for 1 batch) at step 900: 0.4064\n",
            "Seen so far: 28832 samples\n",
            "Training loss (for 1 batch) at step 1000: 0.2410\n",
            "Seen so far: 32032 samples\n",
            "Training loss (for 1 batch) at step 1100: 0.3178\n",
            "Seen so far: 35232 samples\n",
            "Training loss (for 1 batch) at step 1200: 0.5101\n",
            "Seen so far: 38432 samples\n",
            "Training loss (for 1 batch) at step 1300: 0.7106\n",
            "Seen so far: 41632 samples\n",
            "Training loss (for 1 batch) at step 1400: 0.6889\n",
            "Seen so far: 44832 samples\n",
            "Training loss (for 1 batch) at step 1500: 1.1218\n",
            "Seen so far: 48032 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4niG7gcEQ4IH"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}